{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "import selenium\n",
    "import time\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerlink = []\n",
    "\n",
    "r = requests.get('https://www.pgatour.com/players.html').text\n",
    "soup = BeautifulSoup(r,'lxml')\n",
    "\n",
    "for i in soup.find_all('a',class_='player-link'):\n",
    "    playerlink.append(f'https://www.pgatour.com{i.get(\"href\")}')\n",
    "\n",
    "demolink = playerlink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pgatour.com/players/player.57865.alexander-yang.html\n"
     ]
    }
   ],
   "source": [
    "print (demolink[820])\n",
    "# print(len(demolink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got url..\n",
      "Scraping 648\n",
      "done\n",
      "\n",
      "got url..\n",
      "Scraping 649\n",
      "done\n",
      "\n",
      "got url..\n",
      "Scraping 650\n",
      "Would you like to continue..... (y or n)n\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "chrome_path = \"C:\\\\Users\\\\mustafa\\\\Desktop\\\\chromedriver.exe\"\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"start-maximized\");\n",
    "options.add_argument(\"disable-infobars\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "file  = r\"C:\\Users\\mustafa\\Desktop\\PGATOUR.csv\"\n",
    "\n",
    "# Error :- 56,90,118,173,240,259,276,295,345,364,372,407,417,421,432,447,507,510,577,592,595,612,630,643,647,659,666,672,765,773,775,819,820\n",
    "\n",
    "# Scrapped till 650\n",
    "x = 651\n",
    "for link in demolink[651:]:\n",
    "    \n",
    "    if not os.path.exists(file):\n",
    "        csv_file = open(file,'w')\n",
    "        csv_writer = csv.writer(csv_file,lineterminator='\\n')\n",
    "        csv_writer.writerow(['NAME','YEAR','EVENTS PLAYED','1ST','2ND','3RD','TOP 10','TOP 25','MADE CUT','CUT','WD','OFFICIAL MONEY','POINTS','STANDINGS'])\n",
    "    else:\n",
    "        csv_file = open(file,'a')\n",
    "        csv_writer = csv.writer(csv_file,lineterminator='\\n')\n",
    "        \n",
    "    driver = webdriver.Chrome(options=options,executable_path=chrome_path)\n",
    "    driver.get(link)\n",
    "    print(\"got url..\")\n",
    "\n",
    "    time.sleep(9)\n",
    "    WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"performance\"]/div/div[1]/div[2]/ul/li[2]/a'))).click()\n",
    "    \n",
    "    time.sleep(7)\n",
    "    print(f'Scraping {x}') \n",
    "    html = driver.page_source\n",
    "    \n",
    "    playersoup = BeautifulSoup(html,'lxml')\n",
    "    Tables = playersoup.find('div',class_='career-table')\n",
    "    \n",
    "    \n",
    "    #for getting the names of the individual player at a time..........................................\n",
    "    name = playersoup.find('span',class_='nameInner')\n",
    "    name = name.text\n",
    "\n",
    "    \n",
    "    #for getting the years from the tables...........................\n",
    "    year = []\n",
    "    semiyear = Tables.find('div',class_='titles')\n",
    "    yearcontent = semiyear.find_all('td')\n",
    "    for j in range(len(yearcontent)- 1):\n",
    "        year.append(yearcontent[j].text)\n",
    "\n",
    "        \n",
    "    #for getting the rest of the content...........................\n",
    "    tabletag = Tables.find('div',class_='holder')\n",
    "    content = tabletag.find('tbody')\n",
    "    maincontent = content.find_all('tr')\n",
    "    \n",
    "    sublist1 = []\n",
    "    for k in maincontent[:len(maincontent)-1]:\n",
    "        sublist = []\n",
    "        for l in k.find_all('td'):\n",
    "            matter = l.text\n",
    "            if matter == '--':\n",
    "                sublist.append(0)\n",
    "            else:\n",
    "                sublist.append(matter)\n",
    "        sublist1.append(sublist)\n",
    "    \n",
    "    \n",
    "    #for updating in csv format.....................................\n",
    "    for num in range(len(year)):\n",
    "        csv_writer.writerow([name,year[num],sublist1[num][0],sublist1[num][1],sublist1[num][2],sublist1[num][3],sublist1[num][4],sublist1[num][5],sublist1[num][6],sublist1[num][7],sublist1[num][8],sublist1[num][9],sublist1[num][10],sublist1[num][11]])\n",
    " \n",
    "\n",
    "    #for checkpoint purpose. If we want to stop the code and want to resume after some time then this condition is for that purpose.\n",
    "    #But condition value we have to put manually. And after putting 'y' the code will work normally and scraping will get resume normally.\n",
    "    #But after putting 'n' then the code will get stop executing and the csv file will get close successfully. But for resuming after sometime,\n",
    "    #we have to manually change the checkpoint value in condition and also change the value of x (which is above for loop) to the resuming value\n",
    "    #and also change the slicing of for loop same as x value.\n",
    "    if x == 827:\n",
    "        take = input(\"Would you like to continue..... (y or n)\")\n",
    "        if take.lower() == 'y':\n",
    "            pass\n",
    "        else:\n",
    "            csv_file.close()\n",
    "            driver.quit()\n",
    "            print('done')\n",
    "            break\n",
    "    elif x == (len(demolink) - 1):  #final checkpoint where program gets over and scraping gets done.\n",
    "        csv_file.close()\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    " \n",
    "    print('done')\n",
    "    print()\n",
    "    x += 1\n",
    "    driver.quit()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
